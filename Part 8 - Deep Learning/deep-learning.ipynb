{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8 Deep Learning\n",
    "## 8.1 Artificial Neural Networks (ANN)\n",
    "### 8.1.1 The Neuron\n",
    "![8-1-1](8-1-1.png)\n",
    "\n",
    "**Input Value**:\n",
    "- The input value contains all of the independent variables (columns) for a single observation.\n",
    "- It should be standardized.\n",
    "\n",
    "**Output Value**:\n",
    "- It can be continuous, binary, and categorical (several output values in the form of dummy variables).\n",
    "\n",
    "The input and output value deals with one observation at every single time.\n",
    "\n",
    "**Synapse**: \n",
    "- It represents the weight for each signal.\n",
    "\n",
    "**Neuron**:\n",
    "- **STEP 1**: Add the weighted sum of all input values $\\sum_{i=1}^m w_ix_i$.\n",
    "- **STEP 2**: Apply the activation function $\\phi(\\sum_{i=1}^m w_ix_i)$.\n",
    "- **STEP 3**: Pass the signal to the next neuron.\n",
    "\n",
    "### 8.1.2 How Do NNs Learn?\n",
    "- **STEP 1**: Fit the input value of each row into the neural network to compute the output value $\\hat{y}$ for each row.\n",
    "- **STEP 2**: Compare the output value $\\hat{y}$ with the actual value $y$ for each row.\n",
    "- **STEP 3**: Adjust the weights $w_1,w_2,\\dots,w_n$.\n",
    "- **STEP 4**: Do this process again and again in order to find the optimal weights to minimize the cost function.\n",
    "\n",
    "### 8.1.3 Gradient Descent\n",
    "- Batch Gradient Descent: Adjust the weights after running all rows.\n",
    "- Stochastic Gradient Descent: Adjust the weights after running every single row.\n",
    "\n",
    "Stochastic gradient descent an avoid converging to a local optimum. \n",
    "\n",
    "### 8.1.4 Training the ANN with Stochastic Gradient Descent\n",
    "- **STEP 1**: Randomly initialize the weights to the small numbers close to $0$ (but not $0$).\n",
    "- **STEP 2**: Input the first observation of your dataset in the input layer, each feature in one input node.\n",
    "- **STEP 3**: Forward-Propagration: from left to right, the neurons are activated in a way that the impact of each neuron's activation is limited by the weights. Propagate the activations until the predicted result $y$.\n",
    "- **STEP 4**: Compare the predicted result to the actual result. Measure the generated error.\n",
    "- **STEP 5**: Forward-Propagration: from right to left, the error is back-propagated. Update the weights according to how much they are responsible for the error. The learning rate decides by how much we update the weights.\n",
    "- **STEP 6**: Repeat Steps 1 to 5 and update the weights after each observation (reinforcement learning); Repeat Steps 1 to 5 but update the weights only after a batch of observations (batch learning).\n",
    "- **STEP 7**: When the whole training set passed through the ANN, that makes an epoch. Redo more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
