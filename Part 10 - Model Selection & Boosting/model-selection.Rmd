---
title: "Part 10 Model Selection"
output: html_notebook
---

## 10.1 k-Fold Cross Validation
- **STEP 1**: Split the training set into $k$ folds.
- **STEP 2**: We train the model on $k-1$ folds and test the model on the last remaining fold.
- **STEP 3**: We then do the iterations and train and test $k$ combinations of the training and test set in total. 
- **STEP 4**: We take the average of the accuracy of the $k$ combinations and calculate the deviation.

```{r}
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
head(dataset)
```

```{r}
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
```

```{r}
# Splitting the dataset into the training and test set
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
```

```{r}
# Feature scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
```

```{r}
# Fitting kernel SVM to the training set
library(e1071)
classifier = svm(formula = Purchased ~ ., 
                 data = training_set, 
                 type = 'C-classification', 
                 kernel = 'radial')
```

```{r}
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Purchased, k = 10)
cv = lapply(folds, function(x) {
  training_fold = training_set[-x, ]
  test_fold = training_set[x, ]
  classifier = svm(formula = Purchased ~ ., 
                   data = training_fold, 
                   type = 'C-classification', 
                   kernel = 'radial')
  y_pred = predict(classifier, newdata = test_fold[-3])
  cm = table(test_fold[, 3], y_pred)
  accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2])
  return(accuracy)
})
mean(as.numeric(cv))
```

## 10.2 Grid Search
```{r}
# Applying grid search to find the best parameters
library(caret)
classifier = train(form = Purchased ~ ., 
                   data = training_set, 
                   method = 'svmRadial')
classifier
```

```{r}
# Select the best parameters
classifier$bestTune
```

## 10.3 XGBoost
For `XGBoost`, feature scaling is not necessary. 
```{r}
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
dataset = dataset[4: 14]
head(dataset)
```

```{r}
# Encoding the categorical data as factor
dataset$Geography = as.numeric(factor(dataset$Geography, 
                                      levels = c('France', 'Spain', 'Germany'), 
                                      labels = c(1, 2, 3)))
dataset$Gender = as.numeric(factor(dataset$Gender, 
                                   levels = c('Female', 'Male'), 
                                   labels = c(1, 2)))
```

```{r}
# Splitting the dataset into the training and test set
library(caTools)
set.seed(123)
split = sample.split(dataset$Exited, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
```

In `xgboost`, we need to specify:
- `data`: the independent variable. Takes `matrix`, `dgCMatrix`, local data file or `xgb.DMatrix`.
- `label`: the response variable.
- `nrounds`: the max number of iterations.

`xgboost` will return the probability for each category.

```{r}
# Fitting XGBoost to the training set
library(xgboost)
classifier = xgboost(data = as.matrix(training_set[-11]), 
                     label = training_set$Exited, 
                     nrounds = 10)
```

```{r}
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Exited, k = 10)
cv = lapply(folds, function(x) {
  training_fold = training_set[-x, ]
  test_fold = training_set[x, ]
  classifier = xgboost(data = as.matrix(training_set[-11]), 
                     label = training_set$Exited, 
                     nrounds = 10)
  y_prob = predict(classifier, newdata = as.matrix(test_fold[-11]))
  y_pred = (y_prob >= 0.5)
  cm = table(test_fold[, 11], y_pred)
  accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2])
  return(accuracy)
})
mean(as.numeric(cv))
```