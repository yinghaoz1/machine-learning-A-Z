shape2 = numbers_of_rewards_0[i] + 1)
if (random_beta > max_random) {
max_random = random_beta
ad = i
}
}
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
if (reward == 1) {
numbers_of_rewards_1[ad] = numbers_of_rewards_1[ad] + 1
} else {
numbers_of_rewards_0[ad] = numbers_of_rewards_0[ad] + 1
}
total_reward = total_reward + reward
}
total_reward
# Implementing Thompson sampling
N = 10000
d = 10
ads_selected = integer(0)
numbers_of_rewards_1 = integer(d)
numbers_of_rewards_0 = integer(d)
total_reward = 0
for (n in 1:N) {
ad = 0
max_random = 0
for (i in 1:d) {
random_beta = rbeta(n = 1,
shape1 = numbers_of_rewards_1[i] + 1,
shape2 = numbers_of_rewards_0[i] + 1)
if (random_beta > max_random) {
max_random = random_beta
ad = i
}
}
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
if (reward == 1) {
numbers_of_rewards_1[ad] = numbers_of_rewards_1[ad] + 1
} else {
numbers_of_rewards_0[ad] = numbers_of_rewards_0[ad] + 1
}
total_reward = total_reward + reward
}
total_reward
# Implementing Thompson sampling
N = 10000
d = 10
ads_selected = integer(0)
numbers_of_rewards_1 = integer(d)
numbers_of_rewards_0 = integer(d)
total_reward = 0
for (n in 1:N) {
ad = 0
max_random = 0
for (i in 1:d) {
random_beta = rbeta(n = 1,
shape1 = numbers_of_rewards_1[i] + 1,
shape2 = numbers_of_rewards_0[i] + 1)
if (random_beta > max_random) {
max_random = random_beta
ad = i
}
}
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
if (reward == 1) {
numbers_of_rewards_1[ad] = numbers_of_rewards_1[ad] + 1
} else {
numbers_of_rewards_0[ad] = numbers_of_rewards_0[ad] + 1
}
total_reward = total_reward + reward
}
total_reward
# Visualizing the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of Ads Selections (Thompson Sampling)',
xlab = 'Ads',
ylab = 'Number of Times Each Ad Was Selected')
setwd("~/Desktop/Machine Learning A-Z/Part 10 - Model Selection & Boosting")
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
head(dataset)
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:]
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
head(dataset)
# Splitting the dataset into the training and test set
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, spiit == FALSE)
# Splitting the dataset into the training and test set
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature scaling
training_set[-1] = scale(traiing_set[-1])
# Feature scaling
training_set[-1] = scale(training_set[-1])
test_set[-1] = scale(test_set[-1])
# Fitting kernel SVM to the training set
library(e1071)
classifier = svm(formula = Purchased ~ .,
data = training_set,
type = 'C-classification',
kernel = 'radial')
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
head(dataset)
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the training and test set
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature scaling
training_set[-1] = scale(training_set[-1])
# Fitting kernel SVM to the training set
library(e1071)
classifier = svm(formula = Purchased ~ .,
data = training_set,
type = 'C-classification',
kernel = 'radial')
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
head(dataset)
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the training and test set
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature scaling
training_set[-1] = scale(training_set[-1])
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
head(dataset)
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the training and test set
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature scaling
training_set[-1] = scale(training_set[-1])
# Splitting the dataset into the training and test set
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
# Fitting kernel SVM to the training set
library(e1071)
classifier = svm(formula = Purchased ~ .,
data = training_set,
type = 'C-classification',
kernel = 'radial')
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Purchased, k = 10)
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Purchased, k = 10)
cv = lapply(folds, function(x) {
training_fold = training_set[-x, ]
test_fold = training_set[x, ]
classifier = svm(formula = Purchased ~ .,
data = training_fold,
type = 'C-classification',
kernel = 'radial')
y_pred = predict(classifier, newdata = test_fold[-3])
cm = table(test_fold[, 3], y_pred)
accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2])
return(accuracy)
})
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Purchased, k = 10)
cv = lapply(folds, function(x) {
training_fold = training_set[-x, ]
test_fold = training_set[x, ]
classifier = svm(formula = Purchased ~ .,
data = training_fold,
type = 'C-classification',
kernel = 'radial')
y_pred = predict(classifier, newdata = test_fold[-3])
cm = table(test_fold[, 3], y_pred)
accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2])
return(accuracy)
})
cv
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Purchased, k = 10)
cv = lapply(folds, function(x) {
training_fold = training_set[-x, ]
test_fold = training_set[x, ]
classifier = svm(formula = Purchased ~ .,
data = training_fold,
type = 'C-classification',
kernel = 'radial')
y_pred = predict(classifier, newdata = test_fold[-3])
cm = table(test_fold[, 3], y_pred)
accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2])
return(accuracy)
})
cv
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Purchased, k = 10)
cv = lapply(folds, function(x) {
training_fold = training_set[-x, ]
test_fold = training_set[x, ]
classifier = svm(formula = Purchased ~ .,
data = training_fold,
type = 'C-classification',
kernel = 'radial')
y_pred = predict(classifier, newdata = test_fold[-3])
cm = table(test_fold[, 3], y_pred)
accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2])
return(accuracy)
})
cv
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Purchased, k = 10)
cv = lapply(folds, function(x) {
training_fold = training_set[-x, ]
test_fold = training_set[x, ]
classifier = svm(formula = Purchased ~ .,
data = training_fold,
type = 'C-classification',
kernel = 'radial')
y_pred = predict(classifier, newdata = test_fold[-3])
cm = table(test_fold[, 3], y_pred)
accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2])
return(accuracy)
})
cv
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Purchased, k = 10)
cv = lapply(folds, function(x) {
training_fold = training_set[-x, ]
test_fold = training_set[x, ]
classifier = svm(formula = Purchased ~ .,
data = training_fold,
type = 'C-classification',
kernel = 'radial')
y_pred = predict(classifier, newdata = test_fold[-3])
cm = table(test_fold[, 3], y_pred)
accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2])
return(accuracy)
})
mean(as.numeric(cv))
library(caret)
classifier = train(form = Purchased ~ .,
data = training_set,
method = 'svmRadial')
# Applying grid search to find the best parameters
library(caret)
classifier = train(form = Purchased ~ .,
data = training_set,
method = 'svmRadial')
classifer
# Applying grid search to find the best parameters
library(caret)
classifier = train(form = Purchased ~ .,
data = training_set,
method = 'svmRadial')
classifier
# Applying grid search to find the best parameters
library(caret)
classifier = train(form = Purchased ~ .,
data = training_set,
method = 'svmRadial')
classifier
# Select the best parameters
classifier$bestTune
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
head(dataset)
v')
head(dataset)
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
head(dataset)
# Encoding the categorical data as factor
dataset$Geography = as.numeric(factor(dataset$Geography,
levels = c('France', 'Spain', 'Germany'),
labels = c(1, 2, 3)))
dataset$Gender = as.numeric(factor(dataset$Gender,
levels = c('Female', 'Male'),
labels = c(1, 2)))
# Splitting the dataset into the training and test set
library(caTools)
set.seed(123)
split = sample.split(dataset$Exited, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Fitting XGBoost to the training set
install.packages('xgboost')
# Fitting XGBoost to the training set
library(xgboost)
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
dataset = dataset[4: 14]
head(dataset)
# Encoding the categorical data as factor
dataset$Geography = as.numeric(factor(dataset$Geography,
levels = c('France', 'Spain', 'Germany'),
labels = c(1, 2, 3)))
dataset$Gender = as.numeric(factor(dataset$Gender,
levels = c('Female', 'Male'),
labels = c(1, 2)))
# Splitting the dataset into the training and test set
library(caTools)
set.seed(123)
split = sample.split(dataset$Exited, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Fitting XGBoost to the training set
library(xgboost)
classifier = xgboost(data = training_set[])
# Fitting XGBoost to the training set
library(xgboost)
classifier = xgboost(data = as.matrix(training_set[-11]),
label = training_set$Exited)
# Fitting XGBoost to the training set
library(xgboost)
classifier = xgboost(data = as.matrix(training_set[-11]),
label = training_set$Exited,
nrounds = 10)
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Purchased, k = 10)
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Exited k = 10)
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Exited, k = 10)
cv = lapply(folds, function(x) {
training_fold = training_set[-x, ]
test_fold = training_set[x, ]
classifier = svm(formula = Purchased ~ .,
data = training_fold,
type = 'C-classification',
kernel = 'radial')
y_pred = predict(classifier, newdata = test_fold[-3])
cm = table(test_fold[, 3], y_pred)
accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2])
return(accuracy)
})
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Exited, k = 10)
cv = lapply(folds, function(x) {
training_fold = training_set[-x, ]
test_fold = training_set[x, ]
classifier = svm(formula = Exited ~ .,
data = training_fold,
type = 'C-classification',
kernel = 'radial')
y_pred = predict(classifier, newdata = test_fold[-3])
cm = table(test_fold[, 3], y_pred)
accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2])
return(accuracy)
})
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Exited, k = 10)
cv = lapply(folds, function(x) {
training_fold = training_set[-x, ]
test_fold = training_set[x, ]
classifier = xgboost(data = as.matrix(training_set[-11]),
label = training_set$Exited,
nrounds = 10)
y_pred = predict(classifier, newdata = test_fold[-3])
cm = table(test_fold[, 3], y_pred)
accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2])
return(accuracy)
})
# Fitting XGBoost to the training set
library(xgboost)
classifier = xgboost(data = as.matrix(training_set[-11]),
label = training_set$Exited,
nrounds = 10)
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Exited, k = 10)
cv = lapply(folds, function(x) {
training_fold = training_set[-x, ]
test_fold = training_set[x, ]
classifier = xgboost(data = as.matrix(training_set[-11]),
label = training_set$Exited,
nrounds = 10)
y_prob = predict(classifier, newdata = test_fold[-3])
y_pred = (y_prob > 0.5)
cm = table(test_fold[, 3], y_pred)
accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2])
return(accuracy)
})
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Exited, k = 10)
cv = lapply(folds, function(x) {
training_fold = training_set[-x, ]
test_fold = training_set[x, ]
classifier = xgboost(data = as.matrix(training_set[-11]),
label = training_set$Exited,
nrounds = 10)
y_prob = predict(classifier, newdata = test_fold[-3])
y_pred = (y_prob >= 0.5)
cm = table(test_fold[, 3], y_pred)
accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2])
return(accuracy)
})
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Exited, k = 10)
cv = lapply(folds, function(x) {
training_fold = training_set[-x, ]
test_fold = training_set[x, ]
classifier = xgboost(data = as.matrix(training_set[-11]),
label = training_set$Exited,
nrounds = 10)
y_prob = predict(classifier, newdata = as.matrix(test_fold[-11]))
y_pred = (y_prob >= 0.5)
cm = table(test_fold[, 3], y_pred)
accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2])
return(accuracy)
})
mean(as.numeric(cv))
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
dataset = dataset[4: 14]
head(dataset)
# Encoding the categorical data as factor
dataset$Geography = as.numeric(factor(dataset$Geography,
levels = c('France', 'Spain', 'Germany'),
labels = c(1, 2, 3)))
dataset$Gender = as.numeric(factor(dataset$Gender,
levels = c('Female', 'Male'),
labels = c(1, 2)))
# Splitting the dataset into the training and test set
library(caTools)
set.seed(123)
split = sample.split(dataset$Exited, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Fitting XGBoost to the training set
library(xgboost)
classifier = xgboost(data = as.matrix(training_set[-11]),
label = training_set$Exited,
nrounds = 10)
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Exited, k = 10)
cv = lapply(folds, function(x) {
training_fold = training_set[-x, ]
test_fold = training_set[x, ]
classifier = xgboost(data = as.matrix(training_set[-11]),
label = training_set$Exited,
nrounds = 10)
y_prob = predict(classifier, newdata = as.matrix(test_fold[-11]))
y_pred = (y_prob >= 0.5)
cm = table(test_fold[, 3], y_pred)
accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2])
return(accuracy)
})
mean(as.numeric(cv))
# Applying k-fold cross validation
library(caret)
folds = createFolds(training_set$Exited, k = 10)
cv = lapply(folds, function(x) {
training_fold = training_set[-x, ]
test_fold = training_set[x, ]
classifier = xgboost(data = as.matrix(training_set[-11]),
label = training_set$Exited,
nrounds = 10)
y_prob = predict(classifier, newdata = as.matrix(test_fold[-11]))
y_pred = (y_prob >= 0.5)
cm = table(test_fold[, 11], y_pred)
accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2])
return(accuracy)
})
mean(as.numeric(cv))
